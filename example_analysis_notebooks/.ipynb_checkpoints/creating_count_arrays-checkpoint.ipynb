{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12dbf983-0403-4298-97f6-8d9fef7130ac",
   "metadata": {},
   "source": [
    "# Creating Count Arrays\n",
    "\n",
    "Before the analysis of translation limitation can begin the data from the ribosome profiling experiments must be organized into count arrays. Count arrays are vectors that record the number of reads which map to each base pair or codon position along a transcript. The count arrays will be created inside of a Jupyter notebook which is running inside of the Plastid Conda environment set up in (!!!). Using Plastid to create the count arrays will allow for important adjustments to be made to the data such as applying the p-site offsets made in (!!!) and sub-setting the data to only look at the coding regions of the transcripts. The count arrays will be saved as simple csv tables which can be easily incorporated into further analyses in later sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c3c45-529f-44f5-b0b8-790f72cc2825",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 17\n",
    "Load in the python libraries and functions necessary for this pipeline. This includes several functions from plastid and the contents of our setup_utils.py file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198212b6-3fce-430d-b93c-358ff2ae341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from plastid import BAMGenomeArray,GTF2_TranscriptAssembler,Transcript\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plastid.plotting.plots import *\n",
    "import setup_utils as st\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077f1d84-caf2-46fe-9808-41fd7df37cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to important files\n",
    "gtf_path = \"/home/keeganfl/Desktop/Work_Fall_2021/Protocol_test/genome/mouse/\"\n",
    "bam_path = \"/home/keeganfl/Desktop/Work_Fall_2021/Protocol_test/seleno_seq/\"\n",
    "save_path = \"/home/keeganfl/Desktop/Work_Fall_2021/Protocol_test/position_counts_seleno/\"\n",
    "p_site_path = \"/home/keeganfl/Desktop/Work_Fall_2021/data_tables/p-site_offsets/villar/\"\n",
    "experimental = 'Trspfl'\n",
    "samp_num = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83bd9b-3686-4901-92e1-8895346a8b32",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 18\n",
    "Load in the tables of P-site offsets created in the determining p-site offsets section using the Pandas function read_csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02aa05b-ae67-4f35-9f6f-122833357769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the table of P-site offsets. \n",
    "p_offsets_exp =pd.read_csv(p_site_path + experimental + \"_RPF_\" + samp_num + \"_Aligned.toTranscriptome.out_p-site-offsets\", \n",
    "                      sep=\"\\t\")\n",
    "\n",
    "p_offsets_cont =pd.read_csv(p_site_path + \"control\" + \"_RPF_\" + samp_num + \"_Aligned.toTranscriptome.out_p-site-offsets\", \n",
    "                      sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f80bcc-0220-490a-9c4c-441170ef5d28",
   "metadata": {},
   "source": [
    "### Step 19\n",
    "Load in a GTF genome annotation file into python using Plastid’s GTF2_TranscriptAssembler\n",
    "function. This function will load in the transcripts as an iterator of Plastid’s transcript type objects which we will then convert to a list using Python’s list function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6602aae6-6ecf-4e5f-b146-24a11b3409fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the transcript annotations from the GTF file.\n",
    "# GTF2_TranscriptAssembler returns an iterator, so here we convert it to a list.\n",
    "transcripts = list(GTF2_TranscriptAssembler(open(gtf_path + \"mm10.refGene.gtf\"),return_type=Transcript))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b848c-edde-4190-988d-133c2cc47e3c",
   "metadata": {},
   "source": [
    "### Step 20\n",
    "Load in the Bam file containing the Ribosome Profiling data as a Bam Genome Array using Plastid’s BamGenomeArray() function and map the reads to their corresponding P-sites via the VariableThreePrimeMapFactory custom function in setup_utils.py and Plastid’s set_mapping function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec43099-d578-40bc-8286-3a59cabc59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the alignments from a BAM file and then have it map to the p-site \n",
    "alignments_exp = BAMGenomeArray(bam_path + \"subset_\" + experimental + \"_RPF_\" + samp_num + \".bam\")\n",
    "alignments_exp.set_mapping(st.VariableThreePrimeMapFactory(p_offsets=p_offsets_exp))\n",
    "\n",
    "alignments_cont = BAMGenomeArray(bam_path + \"subset_cont\" + \"_RPF_\" + samp_num + \".bam\")\n",
    "alignments_cont.set_mapping(st.VariableThreePrimeMapFactory(p_offsets=p_offsets_cont))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d88bebf-3416-4abb-9c0e-0881c8e8e2ee",
   "metadata": {},
   "source": [
    "### Step 21\n",
    "For each transcript object in our list use Plastid’s get_counts function to create a numpy array that contains the number of counts at each position in the transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc2d803-79d3-4707-a53b-e9db9df0d601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a list to hold the vectors\n",
    "count_vectors_exp = []\n",
    "count_vectors_cont = []\n",
    "\n",
    "# get counts for each transcript\n",
    "for transcript in transcripts:\n",
    "    count_vectors_exp.append(transcript.get_counts(alignments_exp))\n",
    "    count_vectors_cont.append(transcript.get_counts(alignments_cont))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11093c8-e9bb-4bfd-bb97-8c7dfafc602e",
   "metadata": {},
   "source": [
    "### Step 22\n",
    "Once the count arrays have been created the information on CDS regions contained in the transcript type objects can be used to alter the count arrays to only cover the CDS regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba44882-1587-49df-8a82-0e145edeeaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the location of the start and end of the coding region for each transcript. \n",
    "cds_starts = []\n",
    "cds_ends = []\n",
    "\n",
    "for transcript in transcripts:\n",
    "    cds_starts.append(transcript.cds_start)\n",
    "    cds_ends.append(transcript.cds_end)\n",
    "    \n",
    "# Create a list of lists containing the counts at each position of the transcript cds regions.\n",
    "cds_counts_list_exp = []\n",
    "cds_counts_list_cont = []\n",
    "\n",
    "for i in range(len(count_vectors_exp)):\n",
    "    count_vectors_exp[i] = list(count_vectors_exp[i][cds_starts[i]:cds_ends[i]])\n",
    "    count_vectors_cont[i] = list(count_vectors_cont[i][cds_starts[i]:cds_ends[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188bf886-981c-4cd1-b92a-d71c74774e8e",
   "metadata": {},
   "source": [
    "### Step 23\n",
    "Use the add_gene_ids function from setup_utils.py to append the transcript ID and gene ID of each transcript to the start of the count vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4abcf-0ddb-45ce-bb02-874e0ec62fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.add_gene_ids(transcripts, count_vectors_exp)\n",
    "st.add_gene_ids(transcripts, count_vectors_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc892c95-454f-48a3-a49c-249892a44396",
   "metadata": {},
   "source": [
    "### Step 24\n",
    "Filter out any count arrays that are of insufficient length or have insufficient read density. In this example, count arrays which were under 200 base pairs in length or which had a read density below 0.12 reads per base pair were filtered out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2bc109-3153-4a4e-8b03-0e919ab1190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_arrays_exp = []\n",
    "count_arrays_cont = []\n",
    "for array_e, array_c in zip(count_vectors_exp, count_vectors_cont):\n",
    "    if len(array_e) > 200 and sum(array_e[2:])/len(array_e[2:]) > 0.15 and sum(array_c[2:])/len(array_c[2:]) > 0.15:\n",
    "        count_arrays_exp.append(array_e)\n",
    "        count_arrays_cont.append(array_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ba682-aa91-4bb6-8d4c-7ae94b48ef2b",
   "metadata": {},
   "source": [
    "### Step 25\n",
    "Save the count arrays to be used in future notebooks. Use the custom save_count_positions function from setup_utils.py so that the count arrays are saved with a header that describes each column which it is easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551e183-a82b-4273-a4f4-9642548c14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.save_count_positions(count_arrays_exp, save_path + experimental + \"_\" + samp_num + '_counts.csv')\n",
    "st.save_count_positions(count_arrays_cont, save_path + \"control\" + \"_\" + samp_num + '_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eedb6c3-4f8f-4759-ae9b-887cdecac413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
